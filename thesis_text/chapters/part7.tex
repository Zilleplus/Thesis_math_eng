\chapter{Code analysis of PANOC}
The speed of the Matlab or Python code is not very relevant as it only generates source files, its performance is not linked to the controller's performance. The generated C code on the other hand should have a good performance, as it determines the performance of the actual controller.

It is important to measure where most of the execution time is spent inside the algorithm. As this will indicate where to optimize the code. One way to measure this is by using profilers, in this chapter the results of the GNU profiler and the Intel profiler will be studied.

In order to get consistent results which profilers, the code execution must be long enough. And the problem that is being solved, must be sufficiently hard. So it will not converge in only a few steps. Because if it does converge in just a few steps, the cost function and its gradient will be the dominant cost factor. They will take over 90\% of the execution time, and as they are generated by Casadi, there's not much that can be done.

However if the problem is hard enough to solve, the quasi Newton method will be relevant. The buffer will be filled up, and the actual implementation of the algorithm will influence the performance significantly.

The problems used this chapter is displayed in Figure~\ref{fig:solution nmpc problem analysis}. In order to get consistent results, the execution time must be sufficiently long. This is why the same simulations is executed 500 times sequentially while the profiler runs.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{intel_analysis/solution}
	\caption{Solution nmpc problem analysis}
	\label{fig:solution nmpc problem analysis}
\end{figure}

\section{Intel Profiler}
The first profiler that will be discussed is Intel's Vtune Amplifier. In order to make the profiling easier, only one cost/gradient function will be used for both the cost function and its gradient. This means that evaluating the cost function, also means evaluating the gradient.

\subsection{Hot spot analysis}
The Intel profiler contains a hotspot analysis, which finds the functions that take most time when executing the simulation. The results are displayed in Figure~\ref{fig:hotspot no mkl}.  It's quite clear from these results that the inner product and vector additions are two dominant costs inside the algorithm.

The most dominant cost is the cost function and its gradient, as this function is generated by the casadi library. There's nothing that can be done about it's performance. The PANOC algorithm is implemented in a way that the gradient/cost function is never evaluated twice at the same place.

The inner product and vector additions can be implemented using Intel's MKL math library. These routines are fine-tuned BLAS routines and are one of the fastest on the market. Replacing inner products and vector additions with these routines however does not significantly change the performance of the algorithm (results in firgure~\ref{fig:hotspot with mkl}), as the vectors used in this problem are too small to see a visible change. The BLAS functions just become the dominant cost now.

It is true that the aim of the nmpc-codegen library is to provide controller software for embedded devices. And embedded devices will most likely not have BLAS routines. Nevertheless this proves that putting a lot of time in optimizing the linear algebra routines might not always be very productive. The question is, how can the dependency on inner products and vector additions be reduced? The Intel profiler provides a function called bottom-up analysis. Bottom-up visualizes, who is calling all these inner products and vector additions.

%The trailer model uses sines and cosines, these are also expensive operations. More on the sine and cosines later on.
\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=1.2\textwidth]{intel_analysis/no_mkl}
		\caption{no mkl}
		\label{fig:hotspot no mkl}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=1.2\textwidth]{intel_analysis/with_mkl}
		\caption{with mkl}
		\label{fig:hotspot with mkl}
	\end{subfigure}
	\caption{Hot spot analysis with Intel Vtune Amplifier 2018}
\end{figure}

Figure~\ref{fig:bottom-up analysis} provides a high-level view of the bottom-up analysis, Figure~\ref{fig:detailed bottom-up analysis} provides a detailed view of the bottom-up analysis. From the detailed view it is clear that most of the inner product and vector addition function calls are coming from the L-BFGS algorithm. This underlines the importance of choosing the right buffer size for the quasi Newton method. To further illustrate this, Figure~\ref{fig:Simulations with different buffer sizes} contains the simulation results for different buffer sizes.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{intel_analysis/bottum_up}
	\caption{Bottom-up analysis}
	\label{fig:bottom-up analysis}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{intel_analysis/bottum_up_detailed}
	\caption{Detailed bottom-up analysis}
	\label{fig:detailed bottom-up analysis}
\end{figure}

The number of iterations till convergence is illustrated in Figure~\ref{fig:Simulations with different buffer sizes}, if no L-BFGS is used, the algorithm clearly needs significantly more iterations. However if the L-BFGS method is used it will decrease the amount iterations till convergence. The buffer size can be quiet small, as it doesn't seem to improve the performance when the size is greater then 10. This is visible from Figure~\ref{fig:different lbfgs buffer iterations till convergence},  where it is clear that a buffer size of 2, 7 or 200 can result in about the same convergence time.

The bigger the size of the L-BFGS buffer the more inner products and vector additions are required in each step. It is imperative to not only take a large enough buffer, but also not take it too large as this can significantly impact the performance.

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=1.2\textwidth]{compare_lbfgs_size/time_till_convergence}
		\caption{different L-BFGS buffer time till convergence}
		\label{fig:different lbfgs buffer time till convergence}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.45\textwidth}
		\centering
		\includegraphics[width=1.2\textwidth]{compare_lbfgs_size/number_of_iterations}
		\caption{different L-BFGS buffer iterations till convergence}
		\label{fig:different lbfgs buffer iterations till convergence}
	\end{subfigure}
	\caption{Simulations with different buffer sizes}
	\label{fig:Simulations with different buffer sizes}
\end{figure}

The opposite of a bottum-up  analysis is a top down analysis, which determines how  much time the  function spends inside the functions it calls. The results of this analysis is illustrated in Figure~\ref{fig:no mkl top down}. Figure~\ref{fig:no mkl top down} indicates that about one third of the execution time is spend calculating the direction of the quasi Newton method. The two most dominant costs when calculating this direction are the inner products and the vector addition.

The rest of the time is mostly spent calculating the gradient and cost function. The proximal gradient method, is not so dependent upon inner products and vector additions. This is also visible in the caller callee analysis displayed in table~\ref{tbl:Caller Callee analysis}, about 61\% of the time is spent calculating the gradient. If the problem is easier to solve, or the buffer is smaller, then the cost of the gradient is even more dominant. And can easily go up to 80\% or 90\%. This is why a hard problem was used when profiling the algorithm. If an easy problem was used, then the results become irrelevant as the relative costs of the gradient will make everything else irrelevant.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{intel_analysis/no_mkl_topdown}
	\caption{Top down analysis}
	\label{fig:no mkl top down}
\end{figure}

\begin{center}
	\begin{tabular}{| l | c |}
		\hline
		Function&CPU Time: Total (\%) \\
		\hline
		casadi\_interface\_f\_df&61.2087\% \\
		proximal\_gradient\_descent\_forward\_backward\_envelop&36.9998\% \\
		buffer\_evaluate\_new\_location&34.7744\% \\
		lbfgs\_get\_direction&31.6374\% \\
		proximal\_gradient\_descent\_get\_direction&27.168\% \\
		buffer\_evaluate\_pure\_prox\_location&25.6641\% \\
		inner\_product&17.9297\% \\
		\_\_libm\_sse2\_sincos&16.335\% \\
		vector\_add\_ntimes&12.2935\% \\
		lbfgs\_update\_Hessian&1.74661\% \\
		\hline
	\end{tabular}
	\captionof{table}{Caller/Callee analysis}
	\label{tbl:Caller Callee analysis}
\end{center}

\section{Valgrind callgrind}
The Intel profiler combined with the Visual Studio shell is incredibly easy to use and very productive. However it is a proprietary piece of software, and a rather expensive one. The Intel profiler uses the Intel compiler, which often makes for better results with numerical computations then using the GNU compiler. So it is worth while to investigate the results with the GNU compiler.

One way of profiling with the GNU compiler is using valgrind with the option callgrind. Figure~\ref{fig:callgrid} displays the results of callgrind in a drawing. The relative execution time of the gradient is only 53\% instead of 61\% with the Intel compiler. It confirms the results obtained with the Intel profiler, as the inner products and vector additions are even more dominant here.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{callgrid/graph}
	\caption{Callgrid drawing}
	\label{fig:callgrid}
\end{figure}

\section{Conclusion}
The user should be careful when selecting the size of the buffer. If the buffer is too small the convergence will be slow. If the buffer is too large the quasi Newton method will require all lot more inner products and vector additions, which will significantly increase the time to convergence.

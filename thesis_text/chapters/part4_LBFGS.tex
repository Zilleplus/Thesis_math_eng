\chapter{L-BFGS}
	In this chapter the implementation of solving equation~\ref{eq:LBFGS problem} with L-BFGS is discussed. A naive implementation can sometimes lead to strange behavior. So it's import to spend little time on the L-BFGS.
	
	
	\begin{equation}
	R(x) = \frac{1}{\gamma}\left[ x - \prox_g( x - \nabla f(x)\gamma) \right] = 0
	\label{eq:LBFGS problem}
	\end{equation}
	
	\section{Exact newton method}
	The exact Newton method can be derived from a Taylor expansion. It's quadratic convergence makes it a popular choice for solving optimization problems. However convergence is not guaranteed and only quadratic if it is close enough to the solution. The Hessian matrix must be available to the algorithm, and is used in each iteration of the algorithm to solve a system.
	
		\begin{equation}
			g(x_{k+1}) = g(x_k) + \nabla g(x_k)(x_{k+1}-x_k)
			\label{eq:Taylor expansion}
		\end{equation}
		
		\begin{equation}
			x_{k+1}-x_k = -(\nabla g(x_k))^{-1} \cdot g(x_k)
			\label{eq:Taylor expansion reshaped}
		\end{equation}
		
	The Taylor expansion in point $x^k$ can be written as equation~\ref{eq:Taylor expansion}, if $g(x^{k+1})$ is set to zero, the equation becomes equation~\ref{eq:Taylor expansion reshaped}. This leads to equation~\ref{eq:newton method} where $g(x)=\nabla f(x)$ and the step towards zero is p. Typically some sort of line search used to accelerate convergence, so the next location is found using equation~\ref{eq:newton method linesearch} with alpha being the line search parameter.	
	
		\begin{equation}
			p = x_{k+1}-x_k = -(\nabla^2 f(x_k))^{-1} \cdot \nabla f(x_k)
			\label{eq:newton method}	
		\end{equation}
	
		\begin{equation}
		 	x_{k+1} = x_k + \alpha p
		 	\label{eq:newton method linesearch}
		\end{equation}
		
	\section{Quasi Newton Method:BFGS}
	As mentioned in the previous sub section in order to use the exact Newton method the hessian matrix must be available. Sometimes this matrix exists but is not analytically available. It might be hard to express analytically. In this case it is possible to approximate the Hessian matrix as illustrated in equation~\ref{eq:quasi newton method approx Hessian}. This leads to the secant condition equation~\ref{eq:secant condition}. One popular formulation that satisfies the secant condition is that of the BFGS algorithm.
		
		\begin{equation}
			B_{k+1}(x_{k+1}-x_k) = \nabla f(x_{k+1}) - \nabla f(x_k)
			\label{eq:quasi newton method approx Hessian}	
		\end{equation}
		
		\begin{eqnarray}
			s_k = x_{k+1} - x_{k} \\
			y_k = \nabla f(x_{k+1}) - \nabla f(x_{k}) \\
			\rho_k = \frac{1}{y_k^T \cdot s_k}
		\end{eqnarray}
	
		
		\begin{equation}
			B_{k+1} s_{k} = y_{k}
			\label{eq:secant condition}
		\end{equation}
		
		\begin{equation}
			B_{k+1} = B_{k} + \frac{y_k y_k^T}{ y_k^T s_k} - \frac{B_k s_k s_k^T B_k^T}{s_k^TB_ks_k}
			\label{eq:quasi newton method approx Hessian with past values}
		\end{equation}
	Calculating the inverse of the B matrix is an expensive operation (In the order of $O^3$ ). Equation~\ref{eq:quasi newton method inverse hessian} expresses the inverse of the Hessian in a function of the past function values. In every iteration the step is now calculated using equation~\ref{eq:quasi newton method} , the new location used to update the inverse hessian using equation~\ref{eq:quasi newton method inverse hessian}.
		\begin{equation}
			V_k = I - \rho_ky_ks_k^T
		\end{equation}
	
		\begin{equation}
			H^{k+1} = V_k^TH_kV_k + \rho_ks_ks_k^T
			\label{eq:quasi newton method inverse hessian}	
		\end{equation}
		
		\begin{equation}
		p = x_{k+1}-x_k = -(B_k)^{-1} \cdot \nabla f(x_k) = -H_k\cdot \nabla f(x_k)
		\label{eq:quasi newton method}	
		\end{equation}
		
	\section{Quasi Newton Method:L-BGFS}
	A variation on the BFGS algorithm is the L-BFGS algorithm. The L-BFGS algorithm does not express the Hessian matrix explicitly. This way less memory is needed to run the algorithm. Which is very useful if either the problem is very large, or the computer is very small. As nmpc-codegen is aimed at embedded devices the latter is very important.
	
	Equation~\ref{eq:quasi newton method inverse hessian} can be written out recursively as illustrated in equation~\ref{eq:quasi newton method inverse hessian recursive}. Equation~\ref{eq:quasi newton method inverse hessian recursive} can also be expressed in two loops as illustrated in algorithm~\ref{alg:LBFGS}.
	
		\begin{eqnarray}	 
			\begin{aligned}
				H_k = 
				& (V^T_{k-1} ... V^T_{k-m})H^0_k(V^T_{k-m} ... V_{k-1}) \\
				& + \rho_{k-m} (V^T_{k-1} ... V^T_{k-m+1})s_{k-m}s_{k-m}^T(V^T_{k-m+1} ... V_{k-1}) \\
				& + \rho_{k-m+1} (V^T_{k-1} ... V^T_{k-m+2})s_{k-m+1}s_{k-m+1}^T(V^T_{k-m+2} ... V_{k-1}) \\
				& + ... \\
				& + \rho_{k-1}s_{k-1}s_{k-1}^T
			\end{aligned}
			\label{eq:quasi newton method inverse hessian recursive}
		\end{eqnarray}
		
		\begin{equation}
			H_k^0 = \frac{s_k^Ty_k}{y_k^Ty_k}
			\label{eq:quasi newton method initial Hessian}
		\end{equation}
		
	The initial Hessian $H^0_k$ can be estimated using the most recent s and y values, as illustrated in equation~\ref{eq:quasi newton method initial Hessian}. If the optimal point is a local minimum, it's advisable to check if the new initial Hessian is positive definite. The next location then becomes $x_{k+1} = x_{k}+ \alpha \cdot p$ with alpha being the line search parameter. Typically either a wolf-type or Armijo-type line search is used, however panoc uses neither of these methods so they are not discussed.
	
	\section{Cautious update L-BGFS}
	As mentioned before when updating the initial hessian value it's good practice to check if it's positive. If it is negative, there's no point in updating the Hessian value. However even with this safety mechanism, it is still possible to get bad updates. As in updates that lower the convergence rate.
	
	\begin{equation}
		\frac{y_k^Ts_k}{s_k^Ts_k} \ge \epsilon ||\nabla f(x_k)||^\alpha
		\label{eq:cautious update}
	\end{equation}
	
	\cite{Dong-HuiLi1999} Suggests a cautious update based on the size of the gradient and $\frac{s_k^Ts_k}{s_k^Ts_k}$ as illustrated in equation~\ref{eq:cautious update}. The two parameters epsilon and alpha are both positive constants. The cautious update of equation~\ref{eq:cautious update} will be relevant when the algorithm is near fixed point. And the step size is so small the function values hardly change. The information in $s_k$ and $y_k$ is mostly noise and cannot be trusted.
	
	\subsection{L-BFGS implementation}
		The L-BFGS algorithm(algorithm~\ref{alg:LBFGS})is used to solve equation~\ref{eq:LBFGS problem} is. The direction is equal to the proximal gradient step if the buffer is empty. And current\_buffersize will increase every valid update from 1 to the limit specified by the controller designer.
		
		\begin{equation}
			R(x) = \frac{1}{\gamma}\left[ x - \prox_g( x - \nabla f(x)\gamma) \right] = 0
			\tag{\ref{eq:LBFGS problem} revisited}
		\end{equation}
		
		\begin{algorithm}
			\caption{LBFGS}
			\label{alg:LBFGS}
			\begin{algorithmic}[1]
				\Procedure {LBFGS}{$x^k$,M=current\_buffersize}
				\State $q = R(x^k)$
				\For{i=M:1}
				\State $\alpha(i)=\rho(i) \cdot s(:,i)^Tq$
				\State $q = q - \alpha(i) \cdot y(:,i)$
				\EndFor
				\State $H_k^0 = y(:,M) \cdot s(:,M)^T \cdot  \frac{1}{y(:,M)^T \cdot y(:,M)}$
				\State $H^0_k \cdot R(x^k)$
				\For{i=1:M}
				\State $\beta(i) = \rho(i) \cdot y(:,i)^T \cdot z$
				\State $z = z + s(:,i)[\alpha(i)-\beta(i)]$
				\EndFor
				\For{i=1:M-1}
				\State $s(:,i+1)=s(:,i)$
				\State $y(:,i+1)=y(:,i)$
				\EndFor
				\State $$\begin{cases}
				s(:,1) = x_{k+1} - x_k \\
				y(:,1) = \nabla f(x_{k+1}) - \nabla f(x_k)\\
				\rho_k(1) = \frac{1}{y(:,1)^T \cdot s(:,1)} \\ 
				\end{cases}
				$$
				\State \Return direction=$-z=-H_k \cdot R(x^k)$
				\EndProcedure
			\end{algorithmic}
		\end{algorithm}
	
	\subsection{Notes on implementing L-BFGS with panoc}
	Even though the papers on the panoc algorithm sudgest that the problem solved with L-BFGS is equation~\ref{eq:LBFGS problem}. Using $R'(x) = \left[ x - \prox_g( x - \nabla f(x)\gamma) \right]$ with the L-BFGS will give better numerical results.
	
	\begin{equation}
	R(x) = \frac{1}{\gamma}\left[ x - \prox_g( x - \nabla f(x)\gamma) \right] = 0
	\tag{\ref{eq:LBFGS problem} revisited}
	\end{equation}
	
	Every time the search parameter $\gamma$ changes the L-BFGS buffer must be flushed. Because the residual changes if $\gamma$ changes. 
\chapter{L-BFGS}
	In this chapter the implementation of solving \eqref{eq:LBFGS problem} with L-BFGS is discussed. A naive implementation can lead to strange behavior. So it's important to spend time on the L-BFGS implementation.
	
	
	\begin{equation}
	R(x) = \frac{1}{\gamma}\left[ x - \prox_g( x - \nabla f(x)\gamma) \right] = 0
	\label{eq:LBFGS problem}
	\end{equation}
	
	\section{Exact newton method}
	The exact Newton method can be derived from a Taylor expansion. It's quadratic convergence makes it a popular choice for solving optimization problems. However convergence is not guaranteed and only quadratic if it is close enough to the solution. The Hessian matrix must be available to the algorithm, and is used in each iteration of the algorithm to solve a system.
	
		\begin{equation}
			g(x_{k+1}) = g(x_k) + \nabla g(x_k)(x_{k+1}-x_k)
			\label{eq:Taylor expansion}
		\end{equation}
		
		\begin{equation}
			x_{k+1}-x_k = -(\nabla g(x_k))^{-1} \cdot g(x_k)
			\label{eq:Taylor expansion reshaped}
		\end{equation}
		
	The Taylor expansion in point $x^k$ can be written as \eqref{eq:Taylor expansion}, if $g(x^{k+1})$ is set to zero, the equation becomes \eqref{eq:Taylor expansion reshaped}. This leads to \eqref{eq:newton method} where $g(x)=\nabla f(x)$ and the step towards zero is p. Typically some sort of line search used to accelerate convergence, so the next location is found using \eqref{eq:newton method linesearch}, with linesearch parameter.	
	
		\begin{equation}
			p = x_{k+1}-x_k = -(\nabla^2 f(x_k))^{-1} \cdot \nabla f(x_k)
			\label{eq:newton method}	
		\end{equation}
	
		\begin{equation}
		 	x_{k+1} = x_k + \alpha p
		 	\label{eq:newton method linesearch}
		\end{equation}
		
	\section{Quasi Newton Method:BFGS}
	As mentioned in the previous section in order to use the exact Newton method the hessian matrix must be available in practice. Sometimes this matrix exists but is not analytically available. It might be hard to express analytically, or too large to save explicitly in memory. In either case it is possible to approximate the Hessian matrix as illustrated in \eqref{eq:quasi newton method approx Hessian}.  This leads to the secant condition \eqref{eq:secant condition}. One popular formulation that satisfies the secant condition is the BFGS algorithm.
		
		\begin{equation}
			B_{k+1}(x_{k+1}-x_k) = \nabla f(x_{k+1}) - \nabla f(x_k)
			\label{eq:quasi newton method approx Hessian}	
		\end{equation}
		
		\begin{eqnarray}
			s_k = x_{k+1} - x_{k} \\
			y_k = \nabla f(x_{k+1}) - \nabla f(x_{k}) \\
			\rho_k = \frac{1}{y_k^T \cdot s_k}
		\end{eqnarray}
	
		
		\begin{equation}
			B_{k+1} s_{k} = y_{k}
			\label{eq:secant condition}
		\end{equation}
		
		\begin{equation}
			B_{k+1} = B_{k} + \frac{y_k y_k^T}{ y_k^T s_k} - \frac{B_k s_k s_k^T B_k^T}{s_k^TB_ks_k}
			\label{eq:quasi newton method approx Hessian with past values}
		\end{equation}
	Calculating the inverse of the B matrix is an expensive operation. Solving a system of linear equations has a complexity of $\mathcal{O}(n)^3$ ). \eqref{eq:quasi newton method inverse hessian} Expresses the inverse of the Hessian in a function of the past values. In every iteration the L-BFGS step is calculated using \eqref{eq:quasi newton method}. The new location can now be used to update the inverse hessian using \eqref{eq:quasi newton method inverse hessian}.
		\begin{equation}
			V_k = I - \rho_ky_ks_k^T
		\end{equation}
	
		\begin{equation}
			H^{k+1} = V_k^TH_kV_k + \rho_ks_ks_k^T
			\label{eq:quasi newton method inverse hessian}	
		\end{equation}
		
		\begin{equation}
		p = x_{k+1}-x_k = -(B_k)^{-1} \cdot \nabla f(x_k) = -H_k\cdot \nabla f(x_k)
		\label{eq:quasi newton method}	
		\end{equation}
		
	\section{Quasi Newton Method:L-BGFS}
	One variation of the BFGS algorithm is the L-BFGS algorithm. The L-BFGS algorithm does not express the Hessian matrix explicitly. This way less memory is required to run the algorithm. Which is very useful if either the problem is large, or the memory is small. As nmpc-codegen is aimed at embedded devices the latter is very important.
	
	\eqref{eq:quasi newton method inverse hessian} can be written out recursively as illustrated in \eqref{eq:quasi newton method inverse hessian recursive}. \eqref{eq:quasi newton method inverse hessian recursive} Can also be expressed in two loops as illustrated in algorithm~\ref{alg:LBFGS}.
	
		\begin{eqnarray}	 
			\begin{aligned}
				H_k = 
				& (V^T_{k-1} ... V^T_{k-m})H^0_k(V^T_{k-m} ... V_{k-1}) \\
				& + \rho_{k-m} (V^T_{k-1} ... V^T_{k-m+1})s_{k-m}s_{k-m}^T(V^T_{k-m+1} ... V_{k-1}) \\
				& + \rho_{k-m+1} (V^T_{k-1} ... V^T_{k-m+2})s_{k-m+1}s_{k-m+1}^T(V^T_{k-m+2} ... V_{k-1}) \\
				& + ... \\
				& + \rho_{k-1}s_{k-1}s_{k-1}^T
			\end{aligned}
			\label{eq:quasi newton method inverse hessian recursive}
		\end{eqnarray}
		
		\begin{equation}
			H_k^0 = \frac{s_k^Ty_k}{y_k^Ty_k}
			\label{eq:quasi newton method initial Hessian}
		\end{equation}
		
	The initial Hessian $H^0_k$ can be estimated using the most recent s and y values, as illustrated in \eqref{eq:quasi newton method initial Hessian}. If the optimal point is a local minimum, it's advisable to check if the new initial Hessian is positive definite. The next location then becomes $x_{k+1} = x_{k}+ \alpha \cdot p$ with alpha being the line search parameter. Typically either a wolf-type or Armijo-type line search is used, however PANOC uses neither of these methods so they are not discussed.
	
	\section{Cautious update L-BGFS}
	As mentioned before when updating the initial hessian value, it's good practice to check if it's positive. If it is negative, there's no point in updating the Hessian value. However even with this safety mechanism, it is still possible to get bad updates. As in updates that lower the convergence rate of future L-BFGS steps.
	
	\begin{equation}
		\frac{y_k^Ts_k}{s_k^Ts_k} \ge \epsilon ||\nabla f(x_k)||^\alpha
		\label{eq:cautious update}
	\end{equation}
	
	\cite{Dong-HuiLi1999} Suggests a cautious update based on the size of the gradient and $\frac{s_k^Ts_k}{s_k^Ts_k}$ as illustrated in \eqref{eq:cautious update}. The two parameters epsilon and alpha are both positive constants. The cautious update of \eqref{eq:cautious update} will be relevant when the algorithm is near fixed point. And the step size is so small the function values hardly change. The information in $s_k$ and $y_k$ is then dominated noise and cannot be trusted.
	
	\subsection{L-BFGS implementation}
		When using  L-BFGS algorithm(algorithm~\ref{alg:LBFGS}) to solve \eqref{eq:LBFGS problem}. The direction is equal to the proximal gradient step if there is no initial hessian estimate yet. And current\_buffersize will increase every valid update from 1 to the limit specified by the controller designer.
		
		\begin{equation}
			R(x) = \frac{1}{\gamma}\left[ x - \prox_g( x - \nabla f(x)\gamma) \right] = 0
			\tag{\eqref{eq:LBFGS problem} revisited}
		\end{equation}
		
		\begin{algorithm}
			\caption{LBFGS}
			\label{alg:LBFGS}
			\begin{algorithmic}[1]
				\Procedure {LBFGS}{$x^k$,M=current\_buffersize}
				\State $q = R(x^k)$
				\For{i=M:1}
				\State $\alpha(i)=\rho(i) \cdot s(:,i)^Tq$
				\State $q = q - \alpha(i) \cdot y(:,i)$
				\EndFor
				\State $H_k^0 = y(:,M) \cdot s(:,M)^T \cdot  \frac{1}{y(:,M)^T \cdot y(:,M)}$
				\State $H^0_k \cdot R(x^k)$
				\For{i=1:M}
				\State $\beta(i) = \rho(i) \cdot y(:,i)^T \cdot z$
				\State $z = z + s(:,i)[\alpha(i)-\beta(i)]$
				\EndFor
				\For{i=1:M-1}
				\State $s(:,i+1)=s(:,i)$
				\State $y(:,i+1)=y(:,i)$
				\EndFor
				\State $$\begin{cases}
				s(:,1) = x_{k+1} - x_k \\
				y(:,1) = \nabla f(x_{k+1}) - \nabla f(x_k)\\
				\rho_k(1) = \frac{1}{y(:,1)^T \cdot s(:,1)} \\ 
				\end{cases}
				$$
				\State \Return direction=$-z=-H_k \cdot R(x^k)$
				\EndProcedure
			\end{algorithmic}
		\end{algorithm}
	
	\subsection{Notes on implementing L-BFGS with PANOC}
	Even though the papers on the PANOC algorithm sudgest that the problem solved with L-BFGS is \eqref{eq:LBFGS problem}. Using the unnormalized $R'(x) = \left[ x - \prox_g( x - \nabla f(x)\gamma) \right]$ with the L-BFGS will give better numerical results.
	
	\begin{equation}
	R(x) = \frac{1}{\gamma}\left[ x - \prox_g( x - \nabla f(x)\gamma) \right] = 0
	\tag{\eqref{eq:LBFGS problem} revisited}
	\end{equation}
	
	Every time the search parameter $\gamma$ changes the L-BFGS buffer must be flushed.
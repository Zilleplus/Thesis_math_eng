\chapter{PANOC algorithm}
	This section is based on \cite{LorenzoStella2017} and \cite{AjaySathya2017}, the difference is that this text is focused implementation. And so the formula's often look slightly different. The Forward backward envelop part is based on \cite{Themelis}, which contains a slightly different algorithm that also uses the Forward backward envelop.
	\section{Introducing PANOC}
		The PANOC algorithm is an accelerated version of the proximal gradient descent algorithm. The direction  $x-\bar{x}$ is a convex combination of the proximal gradient algorithm's direction and an accelerator that makes use of curvature information. It is formally defined in \cite{LorenzoStella2017} as \eqref{eq:weighting linesearch FBE}, with $\tau_k \leq 1$.
		
		\begin{equation}
			x_{k+1} = x_k + (1-\tau_k)\cdot (x-\bar{x}) + \tau_k d_k
			\label{eq:weighting linesearch FBE}
		\end{equation}
		
		The term $\tau_kd_k$ can accelerate the convergence if $\tau_k\neq0$. The step $d_k$ is calculated using a quasi-newton algorithm. As a quasi-newton method uses curvature information of the cost function, it uses information not available to a gradient descent based method. 
		
		Furthermore it has a super linear convergence rate when it gets close to the solution. Which is much faster then the sub-linear convergence of the gradient descent algorithm . On top of that, the quasi-newton method does not require additional function or gradient evaluations. Which are the major contributors to the computational costs of the algorithm.
		
	\section{Quasi newton method}
		\subsubsection{Problem definition}
			The residue of the iteration \eqref{eq:prox grad method} can be used to solve the optimization problem.  By setting the residue in \eqref{eq:residue prox grad method} to zero, a local optimum can be obtained.
			
			\begin{equation}
			R_{\gamma}(x)= \frac{1}{\gamma}\left[ x - \prox_g( x - \nabla f(x)\gamma) \right]
			\label{eq:residue prox grad method}
			\end{equation}
			
			The root of \eqref{eq:residue prox grad method} can be obtained using the Newton iteration of \eqref{eq:newton iteration residual}. Where $H_k$ satisfies the inverse secant condition of \eqref{eq:newton iteration residual inverse secant}. If the implementation is aimed at embedded software L-BFGS is an excellent choice to solve \eqref{eq:residue prox grad method}. As it doesn't explicitly save the Hessian matrix.
			
			\begin{equation}
			x^{k+1} = x^k -H_kR_{\gamma}(x^k)
			\label{eq:newton iteration residual}
			\end{equation}
			\begin{equation}
			x^{k+1} - x^k = H_{K+1} \Big( R_{\gamma}(x^{k+1})- R_{\gamma}(x^k) \Big)
			\label{eq:newton iteration residual inverse secant}
			\end{equation}
		
	\section{Forward backward envelop}	
		Newton iterations only converge when they are close enough to the solution. In order to get good global behavior, a proper global strategy is required. The optimization problem is changed from $\varphi(x) = f(x) + g(x)$ to \eqref{eq:FBE definition using Moreau envelope}. This problem is smoother while it still has the same optimal solution.(proof see \cite{LorenzoStella2017} and \cite{Themelis}) The same $\gamma$ as with the proximal gradient should be used, notice how the FBE contains the line-search condition used in the proximal gradient. (more on the FBE in \cite{Themelis})
		
		The Moreau envelope is de defined as \eqref{eq:Moreau envelope}, this smooths a function. Using simple algebra \eqref{eq:FBE definition using Moreau envelope} can be transformed into \eqref{eq:FBE definition using quadratic approximation}. The solution $y$ of the infimum in \eqref{eq:FBE definition using quadratic approximation} is $\bar{x}$. Considering the close relationship between the Moreau envelope and the proximal operator this is to be expected. (see more in \cite{Themelis}) 
		
		An alternative way to look at the FBE is illustrated in \cite{AjaySathya2017}, where the problem can be seen as minimizing a quadratic approximation  $f(x) +  \nabla f(x)^T(y-x) + g(y) + \frac{1}{2 \gamma} ||x-y||^2  $ towards y in point x.  Because $L = \frac{1}{\gamma}$, with L as the Lipschitz constant of the gradient.
		
		\begin{equation}
			g^{\gamma} = \underset{y}{\inf} \big \{f(y)+\frac{1}{2 \cdot \gamma}||x-y||^2 \big \}
			\label{eq:Moreau envelope}
		\end{equation}
		
		\begin{equation}
		\varphi_{\gamma} = f(x) - \frac{\gamma}{2}||\nabla f(x)||^2 + g^{\gamma} \big(x-\gamma \nabla f(x) \big)
		\label{eq:FBE definition using Moreau envelope}
		\end{equation}
		
		\begin{equation}
		\varphi_{\gamma} =   f(x) + \underset{y}{\inf} \Big\{ \nabla f(x)^T(y-x) + g(y) + \frac{1}{2 \gamma} ||x-y||^2  \Big\}
		\label{eq:FBE definition using quadratic approximation}
		\end{equation}
		
		\begin{proof}
			The solution to the infimum of \eqref{eq:FBE definition using quadratic approximation} is y=$\bar{x}=\prox_g(x-\gamma \nabla f(x))$
			\begin{align*}
			\prox_g(\bar{x}) 
			&=\prox_g(x- \gamma \nabla f(x)) \\
			&= \underset{y}{\argmin} \Big \{ g(y) 
			+ \frac{1}{2 \gamma}||(y-x) + \gamma \nabla f(x)||^2 \Big \} \\
			&= \underset{y}{\argmin} \Big \{ g(y) 
			+ \frac{1}{2 \gamma} \big[||y-x||^2 + 2 \gamma \nabla f(x)^T(y-x) + ||\nabla f(x)||^2\gamma^2 \big] \Big \} \\
			&= \underset{y}{\argmin} \Big \{ g(y) 
			+ \frac{1}{2 \gamma} \big[||y-x||^2 + 2 \gamma \nabla f(x)^T(y-x)  \big] \Big \}\\
			&= \underset{y}{\argmin} \Big \{   \nabla f(x)^T(y-x)  + g(y) 
			+ \frac{1}{2 \gamma} ||y-x||^2  \Big  \}
			\end{align*}
			\label{prf:prox is solution to FBE inf}
		\end{proof}
		
		\eqref{eq:practical implementation of FBE} Is the practical implementation of the FBE. The parameter gamma is the line-search parameter used in the proximal gradient descent. The first 3 terms are the same as with the line-search on $\gamma$. The last term $g(\bar{x})$ is new and ensures that the solution complies with the constraint.
		
%		\cite{AjaySathya2017} has an excellent example to illustrate this.(Figure~\ref{fig:FBE illustration}).
%		
		\begin{equation}
			\begin{aligned}	
				& \varphi(\gamma,x)= 
				&& f(x) - \nabla f(x)^T(x-\bar{x}) + \frac{1}{2 \gamma}||x-\bar{x}||^2  + g(\bar{x})
				\\
				& with 
				&&\bar{x} = \prox_g( x - \gamma\nabla f(x)) 
			\end{aligned} 
			\label{eq:practical implementation of FBE}
		\end{equation}
		
%		\begin{figure}[H]
%			\centering
%			\label{fig:FBE illustration}
%			\includegraphics[width=0.6\textwidth]{FBE}
%			\caption{FBE example copied over from \cite{AjaySathya2017}, red is?? blue is ?? TODO expain this figure}
%		\end{figure}
	
	\section{Line-search with FBE}
	In \cite{LorenzoStella2017} the line-search condition is specified as equation ~\eqref{eq:line-search with FBE}. The line-search parameter $\tau$ determines $x^{k+1}$ by weighting the convex combination of the proximal gradient step and the L-BGFS step as defined in \eqref{eq:linea-search tau definition}.  \cite{LorenzoStella2017} Specifies that $\sigma \in (0, \gamma \frac{1-\gamma\cdot L}{2})$. As stated before it is assumed that $L=\frac{1-\beta}{\gamma}$, some simple algebra will lead to the condition $\sigma \in (0,\frac{\beta \gamma}{2})$.
	
	 \eqref{eq:practical line-search with FBE} Is the practical implementation of \eqref{eq:line-search with FBE}, it incorporates the safety value $\beta$. A new constant $\alpha \in (0,1)$ is introduced, a possible value for $\alpha$ would be 0.5 ($\alpha=0.5$ is the choice used by the Matlab implementation of PANOC in ForBes known as zerofpr2).
	
	\begin{equation}
		x^{k+1} = u_k - (1-\tau_k)\cdot (x-\bar{x}) + \tau \cdot dir_{LBFGS}
		\label{eq:linea-search tau definition}
	\end{equation}
	
	\begin{eqnarray}
		\label{eq:line-search with FBE}
		\varphi_{\gamma}(x^{k+1})\leq\varphi_{\gamma}(x^{k}) - \sigma ||\frac{x-\bar{x}}{\gamma}||^2 \\
		=
		\varphi_{\gamma}(x^{k}) - \frac{\sigma}{\gamma^2} ||x-\bar{x}||^2
	\end{eqnarray}
	
	\begin{equation}
		\varphi_{\gamma}(x^{k+1}) \leq 		\varphi_{\gamma}(x^{k}) - \alpha \frac{\beta }{\gamma \cdot 2} ||x-\bar{x}||^2
		\label{eq:practical line-search with FBE}
	\end{equation}
	
	
		
		\begin{algorithm}
			\caption{PANOC}
			\label{alg:PANOC}
			\begin{algorithmic}[1]
				\Procedure {PANOC\_GET\_NEW\_LOCATION}{$x^k$,$\gamma$}
				\State [$(x-\bar{x})$ , $\gamma$] = GET\_PROXIMAL\_GRADIENT\_STEP($\gamma$,$x^k$)
				\State $ dir_{LBFGS}$ = LBFGS($x^k$)
				\State $\tau =1$
				\State $x^{k+1} = x_k - (1-\tau_k)\cdot (x-\bar{x}) + \tau \cdot dir_{LBFGS}$
				\While{$\varphi_{\gamma}(x^{k+1}) > 		\varphi_{\gamma}(x^{k}) - \alpha \frac{\beta}{\gamma \cdot 2} ||(x-\bar{x})||^2$}
				\State $\tau = \tau / 2$
				\State $x^{k+1} = x_k - (1-\tau_k)\cdot (x-\bar{x}) + \tau \cdot dir_{LBFGS}$
				\EndWhile
				\EndProcedure
			\end{algorithmic}
		\end{algorithm}
	
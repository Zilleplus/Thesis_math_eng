\chapter{Proximal gradient method}
	\section{Proximal mapping}
		The proximal operator is defined as $\prox_{\gamma g}(x)= \underset{u}{\argmin}(g(u) + \frac{1}{2 \gamma}||u-x||^2_2)$ in \cite{NealParikh}. 
		
		\begin{itemize}
			\item if $h(x)=0$ then $\prox_{\gamma h}(x)=x$ 
			\item if $h(x)=I_c(x)$ where $I_c$ is define in \eqref{eq:indicator function}, the proximity mapping of the indicator function is the orthogonal projection on the set.
		\end{itemize}
		
		The indicator function: is defined in equation ~\eqref{eq:indicator function}.
		\begin{equation}
			I_c = 
			\begin{cases}
			0 & x \in C  \\
			\infty & x \notin C
			\end{cases}
			\label{eq:indicator function}
		\end{equation}
		
		The proximal mapping can be seen as a generalization of a projection. The Appendix contains an other important example, the indicator box function.
	
	\section{Gradient projected method}
		
		\begin{equation}
			\begin{aligned}
			& \underset{x}{\text{argmin}}
			& & f_0(x) \\
			& \text{subject to}
			& & g(x)=0
			\end{aligned}
			\label{eq:prox grad opti problem}
		\end{equation}
		
		The classical gradient decent method cannot be used to solve the problem of \eqref{eq:prox grad opti problem}. As this problem has a condition that must be met by the algorithms solution. If in each iteration the solution is projected on the space spanned by $g(x)=0$ the iteration of \eqref{eq:grad descent} becomes \eqref{eq:projected grad descent}. This algorithm is called the gradient projection method.
		
		\begin{equation}
			x^k = x^{k-1} - \gamma \nabla f(x^{k-1})
			\label{eq:grad descent}
		\end{equation}
		
		\begin{equation}
			x^k = \project_{g(x)=0}[ x^{k-1} - \gamma \nabla f(x^{k-1})]
			\label{eq:projected grad descent}
		\end{equation}
		
		If $g(x)$ is the indicator function of the set onto which is projected. Then  \eqref{eq:projected grad descent} can be written as \eqref{eq:proximal grad descent}, known as the proximal gradient method.
		
		\begin{equation}
				x^k = \prox_{\gamma g}[ x^{k-1} - \gamma \nabla f(x^{k-1})]
			\label{eq:proximal grad descent}
		\end{equation}
	
	\section{The proximal gradient method}
		\eqref{eq:prox grad problem} Can be solved with the proximal gradient method, sometimes called forward backward splitting (FBS) . If the proximal mapping of $g(x)$ is analytically defined. 
			\begin{equation}
			\underset{x}{\argmin} = f(x) + g(x)
			\label{eq:prox grad problem}
			\end{equation}
		
		Inspired by the projected gradient method, the proximal gradient method is defined as \eqref{eq:prox grad method} in \cite{NealParikh}. The $\gamma$ variable is the step size, in order to have a convergence of O(1/k) $\gamma \in(0,1/L)$. If $\gamma \in (1/L,2/L)$ the algorithm will still converge but then its no longer a majorization-minimization method. (more on this see \cite{NealParikh})
		
		\begin{equation}
			x^k = \prox_{\gamma g}\big( x^{(k-1)}- \gamma \nabla f(x^{(k-1)})\big)
			\label{eq:prox grad method}
		\end{equation}	
	
	\section{Proximal minimization algorithm}
		 \cite{QianYang} Contains a short proof that illustrates the proximal mapping as a fixed point minimization algorithm. A property of the conjugated function is used to derive the gradient, see appendix for the theorem.
		 \begin{proof}
		 	The iteration $x^{k+1}=\prox_g(x^k)$ will minimize the smoothed version of function f(x), a shorter version of this proof van be found in \cite{QianYang}. 
		 	\begin{align*}
		 	f_{\mu}
		 	&= \underset{y}{\inf}\Big\{ |y| +\frac{1}{2 \mu}(x-y)^2 \Big\} \\
		 	&=   \frac{1}{2 \mu}||x||^2 + \frac{1}{2\mu} 
		 	\underset{y}{\inf}\Big\{
		 	2 \mu f(y) - 2x^T y + ||y||^2
		 	\Big\} \\
		 	&=  \frac{1}{2 \mu}||x||^2 + \frac{1}{\mu} 
		 	\underset{y}{\sup}\Big\{
		 	x^T y  - \mu f(y) - \frac{1}{2} ||y||^2 \Big \} \\
		 	&= \frac{1}{2\mu }||x||^2 \frac{1}{\mu } \Big( \mu f + \frac{1}{2}||\cdot||^2 \Big)^* (x) \\
		 	\nabla  f_{\mu} 
		 	&= \frac{x}{\mu} - \frac{1}{\mu} \underset{y}{\argmax} 
		 	\Big \{ x^Ty - \mu f(y) - \frac{1}{2}||y||^2 \Big \}\\
		 	& = \frac{1}{\mu}(x - \prox_{\mu f}(x)) \\
		 	\prox_{\mu f}(x)
		 	& = x- \mu \nabla f_{\mu}(x)
		 	\end{align*}
		 	\label{prf:proximal minimiztion alg proof}
		 \end{proof}

		 This means thats the iteration $x^{k+1}=\prox_g(x^k)$ will minimize the smoothed version of function g(x). 

\section{Proximal gradient method with line search}
	\subsection{Starting value gamma }
		\subsubsection{Estimating Lipschitz value}
			The Lipschitz of $\nabla f(x)$ value is a non negative number that complies with \eqref{eq:definition lipschitz value}.
			\begin{equation}
			L = \underset{x \neq y}{\sup} \frac{|\nabla f(y)-\nabla f(x)|}{|y-x|}
			\label{eq:definition lipschitz value}
			\end{equation}
			
			In practice it is not always possible to find the actual Lipschitz value. So the Lipschitz value is estimated ($L'$) locally at the starting location $x_0$ using \eqref{eq:estimated lipschitz value in starting position}. With $\delta=max[\delta_l,10^{-6} \cdot x_0]$ where $\delta_l$ is a small number chosen by the controller designer.
			% in code the safety value is DELTA_LIPSCHITZ_SAFETY_VALUE and set to 10^-6
			
			\begin{equation}
			L' = \frac{|\nabla f(x_0+\delta)-\nabla f(x_0)|}{|\delta|}
			\label{eq:estimated lipschitz value in starting position}
			\end{equation}
			
			The Lipschitz value is not explicitly saved but is used to estimate the line-search parameter $\gamma$. The backtracking of the proximal gradient algorithm then further improves $\gamma$. As the algorithm progresses and $\gamma$ improves, so does the estimation of the Lipschitz value indirectly.
		
		\subsubsection{Estimating gamma}	
			\cite{LorenzoStella2017} states that $\gamma<\frac{1}{L}$ guarantees convergence to a local minimum. As gamma needs to be smaller than $\frac{1}{L}$ a safety value is introduced. This idea was copied over from the kul-forbes/ForBES library by Lorenzo Stella and Panos Patrinos, which uses a $\beta$ of 0.05. And leads to \eqref{eq:starting value gamma}.
			\begin{equation}
			\gamma = \frac{1-\beta}{L}
			\label{eq:starting value gamma}
			\end{equation}		
	
	\subsection{Backtracking gamma}			
		\subsubsection{Backtracking in proximal gradient descent used in FBS}
			The line-search makes use of the quadratic bound of \eqref{eq:Armijo's sufficient decrease condition prox grad PANOC} Which is an quadratic bound, by backtracking on $\gamma$ the optimal step size is retrieved.
			
			\begin{equation}
			f({\bar{x}}) \leq f(x) - \nabla f(x)^T[x-\bar{x}] + \frac{1}{2 \gamma}||x-\bar{x}||^2
			\label{eq:Armijo's sufficient decrease condition prox grad PANOC}
			\end{equation}
			
			This leads to algorithm~\ref{alg:backtracking on gamma} the proximal gradient method. \eqref{eq:Armijo's sufficient decrease condition prox grad PANOC} can be seen as a quadratic model, as the Lipschitz value of the gradient is equal to $L=\frac{1}{\gamma}$.
			
			\begin{algorithm}
				\caption{backtracking $\gamma$}
				\label{alg:backtracking on gamma}
				\begin{algorithmic}[1]
					\Procedure {lineasearch\_gamma}{x,$\gamma$}
					\State $\bar{x} = \prox_{\gamma g}\big( x- \gamma \nabla f(x)\big)$
					\While{$f({\bar{x}}) > f(x) - \nabla f(x)^T[x-\bar{x}] + \frac{1}{2 \gamma}||x-\bar{x}||^2$}
					\State $\gamma = \frac{\gamma}{2}$
					\State $\bar{x} = \prox_{\gamma g}\big( x- \gamma \nabla f(x)\big)$
					\EndWhile
					\State \Return $\gamma$
					\EndProcedure
				\end{algorithmic}
			\end{algorithm}
	\subsection{Final algorthm}
		The final algorithm~\ref{alg:proximal gradient PANOC with backtracking} delivers the upward direction. $x_{k+1}=x_k - direction$.
		\begin{algorithm}
			\caption{proximal gradient PANOC with backtracking}
			\label{alg:proximal gradient PANOC with backtracking}
			\begin{algorithmic}[1]
				\Procedure{get\_proximal\_gradient\_step}{x,$\gamma$}
				\State $\gamma$=LINESEARCH\_GAMMA($\gamma$)
				\State $\bar{x} = \prox_{\gamma g}\big( x- \gamma \nabla f(x)\big)$
				\State \Return direction=$[x-\bar{x}]$, $\gamma$
				\EndProcedure
			\end{algorithmic}
		\end{algorithm}
\section{Proximal gradient alternative view}
	\subsection{Majorization-minimization algorithm}
	A Majorization-minimization algorithm is a type of algorithm that minimizes a surrogate function. The surrogate function is a approximation of the actual problem. And needs to have the conditions described in \eqref{eq:MM algorithm conditions}, where h(x,y) is a surrogate function and f(x) is the actual problem. The problem described in \eqref{eq:MM algorithm formula step} is iteratively solved until convergence.
	\begin{equation}
		\begin{cases}
			h(x,x) = f(x) \\
			h(x,y) \leq f(x)
		\end{cases}
		\label{eq:MM algorithm conditions}
	\end{equation}
	
	\begin{equation}
		x_{k+1} = \argmin_y h(x_k,y)
		\label{eq:MM algorithm formula step}
	\end{equation}

	\subsection{The proximal gradient as majorization-minimization algorithm}
	\cite{NealParikh} States that the proximal gradient algorithm can be seen as a majorization-minimization algorithm. Its surrogate function is described in \eqref{eq:surrogate function}, proof~\ref{prf:proximal gradient as MM} illustrates how minimizing the surrogates function is equal as taking a step of the proximal gradient algorithm.
	
	\begin{equation}
		h(x,y) = f(x) + \nabla f(x)^T(x-y) + \frac{1}{2 \cdot \gamma}||x-y||^2_2
		\label{eq:surrogate function}
	\end{equation}
	
	\begin{proof}
		$h(x,y) = f(x) + \nabla f(x)^T(x-y) + \frac{1}{2 \cdot \gamma}||x-y||^2_2$ is the MM surugate function of proximal gradient algorithm. The gradient is set too zero and solved for y.
		\begin{align*}
		h(x,y)
		& = f(x) + \nabla f(x)^T(x-y) + \frac{1}{2 \cdot \gamma}||x-y||^2_2 \\
		\frac{\partial h(x,y)}{\partial y}
		& = 0 + \nabla f(x) + \frac{1}{2 \cdot \gamma} + \frac{1}{2 \cdot \gamma}\frac{\partial ||x-y||^2_2}{\partial y}  \\	
		\frac{\partial ||x-y||^2_2}{\partial y}
		& = \frac{\partial(x-y)^T(x-y)}{\partial y} = \frac{\partial(-2x^Ty + y^Ty)}{\partial y} = -2x+y \\
		\frac{\partial h(x,y)}{\partial y}
		& = \nabla f(x) - \frac{x}{\gamma} + \frac{y}{2 \gamma} = 0 \\
		0
		& = \gamma \nabla f(x) - x + \frac{y}{2} \\
		y & = \gamma \nabla f(x) - x
		\end{align*}
		\label{prf:proximal gradient as MM}
	\end{proof}
\chapter{Control Systems}
\section{Optimal control}
	The optimal control problem goes back more than 300 years. From Galileo and Newton to Johann Bernoulli and Euler with the brachistochrone problem. The optimal control problem is the problem of finding the proper inputs so that the system behaves in a certain way.
	
	A typical optimal control problem exists out of a set of differential equations that describes the behavior of the system. And a cost function that describes the cost of the specific trajectory integrated on the differential equations. The most optimal solution to the optimal control problem will have the lowest cost possible.
	
	Optimal control problems can be mathematically defined as the cost function equation~\ref{eq:optimal control definition}(With boundary conditions equation~\ref{eq:optimal control definition state equations boundary conditions})  and the behavior of the system equation~\ref{eq:optimal control definition state equations}. In addition constraints can be placed on the state and inputs, as shown by equation~\ref{eq:optimal control definition state equations path constraints}. 
	
	\begin{equation}
		J = S[x(t_0),t_0,x(T),T] + \int_{t_0}^{T} L[x(t),u(t),t]
		\label{eq:optimal control definition}
	\end{equation}
	\begin{equation}
		\dot{x}(t) = F(x(t),u(t),t)
		\label{eq:optimal control definition state equations}
	\end{equation}
	\begin{equation}
		C[x(t),u(t),t]\le 0
		\label{eq:optimal control definition state equations path constraints}
	\end{equation}
	\begin{equation}
		S[x(t_0),t_0,x(T),T]=0
		\label{eq:optimal control definition state equations boundary conditions}
	\end{equation}

\section{MPC}
	Model predictive control is an advanced process control method that continuously solves a series of optimal control problems. At a constant rate the state is measured, a prediction is made with possible future inputs of the future states. The future inputs are changed so that the simulated state is close to the reference state. The input is applied to the system, and the cycle repeats itself.
	
	A physical system can be represented as $\dot{x}=F_c(x,u)$, where x is the current state and u is the current input. By using a discrete integrator the system can be written as $x^{k+1}=F_d(x^{k},u^{k})$. 
	
	Figure~\ref{fig:MPC diagram} is a simple diagram that illustrates MPC. The reference trajectory is the behavior we want, the predicted control input is the control input that was calculated and in the simulations lead to the predicted output.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\textwidth]{MPC_scheme}
		\caption{A simple MPC diagram from the wikipedia page}
		\label{fig:MPC diagram}
	\end{figure}
			
	\subsection{System}
		
	\subsection{Problem definition}
		\subsubsection{Problem form}
			The goal is to define the problem as equation~\ref{eq:PANOC MPC form}, and then solve it for u given $x_0$, the current state of the system. Sometimes $x_0$ will be assumed to be part of the function f and the form of equation~\ref{eq:PANOC form} will be used.
			\begin{equation}
				\underset{u}{\minimize} \  f(x_0,u) + g(u)
				\label{eq:PANOC MPC form}
			\end{equation}
			
			\begin{equation}
				\underset{u}{\minimize} \  f(u) + g(u)
				\label{eq:PANOC form}
			\end{equation}
		\subsubsection{Direct Single shoot}
			If the horizon is N then the problem is solved for the inputs $u=[u_0,u_1,... u_{N-1}]$ where each $u_k$ is a vector of all the inputs of the system. This means that the vector u is of size (Horizon-1)*dimension\_input .
			
			Define the cost for each step in the horizon as \ref{eq:single shot iteration cost}.
			\begin{equation}
				\begin{aligned}
				& l_k(x_0,u) = &&  x_k^T Q x_k  +  u_k^T R u_k \\
				& \text{subject to}			&& x_0 = \bar{x} \\
				& 							&&  x_{n+1} = F_d(x_n,u_n), n=0...N-1
				\end{aligned}
				\label{eq:single shot iteration cost}
			\end{equation}
			
			Define the terminal cost as equation~\ref{eq:single shot terminal cost}.
			\begin{equation}
				\begin{aligned}
					& l_N(x_0,u) = && x_N^TSx_N \\
					& \text{subject to}			&& x_0 = \bar{x} \\
					& 							&&  x_{n+1} = F_d(x_n,u_n), n=0...N-1
				\end{aligned}
				\label{eq:single shot terminal cost}
			\end{equation}
			
			$f(x_0,u)$ can then be defined as in equation~\ref{eq:single shot definition} the sum of the cost of all iterations plus the terminal cost.
			\begin{equation}
				f(x_0,u) = \sum_{k=1}^{N-1} l_k(x_0,u) + l_N
				\label{eq:single shot definition}
			\end{equation}
			
			As a side note, an other term can be added to equation~\ref{eq:single shot definition} to represent the obstacle avoidance.
		\subsection{Direct Multiple shoot}
			In multiple shoot mode the optimization algorithm needs more than just an initial state. It requires a guess of all of the intermediate states that where derived using the initial state in  single shot mode. The initial guesses will be referred to as $x_i$ and the states derived from this initial gas and its corresponding input will be referred to as $\bar{x_i}$. As before the goal of the optimization algorithm is to find the optimal inputs $u_i$, so that $\bar{x_i}$ is optimal.
			
			\begin{equation}
				\bar{x_i} = F(x_i,u_i)
				\label{eq:}
			\end{equation}
			
			In addition to getting the states to a specific reference state, the algorithm also needs continuity conditions. These continuity conditions are displayed in equation~\ref{eq:continuety condition multiple shot} and were not necessary in single shot as every solution already complies with it automatically. If the initial state guesses are relatively close to the solution, a significant speed increase can be accomplished. As we are able to incorporate more information into the algorithm.
			
			\begin{equation}
				\bar{x_i} - x_{i+1} = 0
				\label{eq:continuety condition multiple shot}
			\end{equation}
			
			The final cost definition looks like equation~\ref{eq:multiple shot cost} and has an extra equality condition compared to the single shoot. This equality condition will be added as a soft constraints to the cost function. This is displayed in equation~\ref{eq:multiple shot cost with soft constraint}
			
			\begin{equation}
				\begin{aligned}
				L =  & \sum_{i=1}^{N} l(\bar{x_i},u_i) \\
				& \text{subject to}			&& \bar{x_i} = F(x_i,u_i) \\
				& 							&& \bar{x_i} - x_{i+1} = 0
				\end{aligned}
				\label{eq:multiple shot cost}
			\end{equation}
			
			\begin{equation}
			\begin{aligned}
			L =  & \sum_{i=1}^{N} l(\bar{x_i},u) + ||\bar{x_i} - x_{i+1}||\\
			& \text{subject to}			&& \bar{x_i} = F(x_i,u_i) \\
			\end{aligned}
			\label{eq:multiple shot cost with soft constraint}
			\end{equation}
			
		\subsection{Obstacle avoidance}
			The obstacle avoidance is based on the soft constraint described in \cite{AjaySathya2017}.
			\subsubsection{As set}
				An obstacle can be defined as an open set, as illustrated by equation~\ref{eq:obstacle as open set}. It is defined by the intersection of a number of nonlinear inequalities.
				\begin{equation}
					O = {z \in \Re^d : h^i(z)>0,\ i \in N}
					\label{eq:obstacle as open set}
				\end{equation}
				
			\subsubsection{As constraint}
				\begin{equation}
					[z]_+ =  \max\{0,z\}
				\end{equation}
				
				The statement $h(x)<0$ is equivalent to saying $[h(x)]_+=0$,so equation~ref{eq:obstacle as open set} is equivalent to saying that equation~\ref{eq:obstacle as equality} needs to be zero.
				
				\begin{equation}
					\Phi_0(z) =  \frac{1}{2} \prod_{i=1}^m \Big( [h^i(z)]_+ \Big)^2
					\label{eq:obstacle as equality}
				\end{equation}
				
				The gradient of equation~\ref{eq:obstacle as equality} is define as equation~\ref{eq:obstacle as equality}
				
				\begin{equation}
					\nabla \Phi =
					\begin{cases}
						\sum_{i=1}^{m} h^i(z)\prod_{j \ne i} \Big( [h^i(z)]_+ \Big)^2 \nabla h^i(z)
						& x \in O \\
						0 & else
					\end{cases}
					\label{eq:derivative obstacle as equality}
				\end{equation}
			
			\subsubsection{Polyhedral obstacle}
				A simple obstacle can be defined as equation~\ref{eq:polyhedral constraint} a polyhedral.
				\begin{equation}
					\prod \Big([b_i - a_i^t z]_+ \Big)^2 = 0
					\label{eq:polyhedral constraint}
				\end{equation}
			
			\subsubsection{Obstacle as soft constraint}
				The obstacle avoidance is added as a soft constraint to the cost function. As demonstrated in equation~\ref{eq:derivative obstacle as equality}, the gradient is defined and so this does not break the condition that the cost functions needs a gradient to be solved with the PANOC algorithm.
			
		\subsection{Constraints}
			An other important aspect of a MPC problem are the constraints. In practice inputs have to comply with the physical properties of the devices. Absurdly high or low input values might in theory lead to a fast solution, but are not feasible in practice.
			
			An major advantage of the PANOC algorithm is that it can take non linear or non convex constraints. As longs as the proximal operation is analytically defined on the constraint. 
			
			A simple example is the indicator box function, which allows to set a maximum and minimum value on the inputs. (The indicator box function is defined in the appendix) This means that every feasible solution lies within the bounds of the user defined box.
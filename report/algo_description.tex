\documentclass[]{article}

\usepackage{amsmath}
\newcommand{\argmin}{\arg\!\min}

%\usepackage{program}
\usepackage{algorithm} 
\usepackage{algcompatible}
\usepackage{algpseudocode}

\usepackage[parfill]{parskip} % paragraphs are seperated with an enter space

%opening
\title{A simple introduction to PANOC}
\author{Willem Melis}

\begin{document}

\maketitle

\begin{abstract}
This is a simple introduction to the PANOC algorithm. First the proximal operator is introduced with the  gradient decent algorithm and proximity gradient decent algorithm. Next the link is made between these algorithms and the PANOC algorithm.
\end{abstract}

\section{Proximal gradient method}
	\subsection{Proximal mapping}
		The proximal operator is defined as $prox_g(x)= \underset{u}{argmin}(f(u) + \frac{1}{2}||u-x||^2_2)$. 
		
		\begin{itemize}
			\item if $h(x)=0$ then $prox_h(x)=x$ 
			\item if $h(x)=I_c(x)$ where $I_c$ is define in Equation~\ref{eq:indicator function} the proximity operator is the orthogonal projection
		\end{itemize}
		
		The indicator function: is defined in equation ~\ref{eq:indicator function}.
		\begin{equation}
			I_c = 
			\begin{cases}
			0 & x \in C  \\
			\infty & x \notin C
			\end{cases}
			\label{eq:indicator function}
		\end{equation}
	
	\subsection{Gradient projected method}
		
		\begin{equation}
			\begin{aligned}
			& \underset{x}{\text{argmin}}
			& & f_0(x) \\
			& \text{subject to}
			& & g(x)=0
			\end{aligned}
			\label{eq:prox grad opti problem}
		\end{equation}
		
		The classical gradient decent method cannot be used to solve problem in equation~\ref{eq:prox grad opti problem}. As this problem has a condition that must be met by the algorithms solution. The gradient decent iterations are defined in equation~\ref{eq:grad descent} . If in each iteration the solution is projected on the space spanned by $g(x)=0$ the iterations become equation~\ref{eq:projected grad descent}. This algorithm is called the gradient projected method.
		
		\begin{equation}
			x^k = x^{k-1} - \nabla f(x^{k-1})
			\label{eq:grad descent}
		\end{equation}
		
		\begin{equation}
			x^k = project_{g(x)=0}[ x^{k-1} - \nabla f(x^{k-1})]
			\label{eq:projected grad descent}
		\end{equation}
		
		If $g(x)$ is an indicator function then  equation~\ref{eq:projected grad descent} can be written as 
		
		\begin{equation}
				x^k = prox_{I_c}[ x^{k-1} - \nabla f(x^{k-1})]
			\label{eq:proximal grad descent}
		\end{equation}
	
	\subsection{The proximal gradient method}
		The problem solved by the proximal gradient method (sometimes called forward backward splitting - FBS) is equation~\ref{eq:prox grad problem}. Where the proximal operator with $g(x)$ is easily computed. 
			\begin{equation}
			\underset{x}{\text{argmin}} = f(x) + g(x)
			\label{eq:prox grad problem}
			\end{equation}
		
		Inspired by the projected gradient method the proximal gradient method is define in equation~\ref{eq:prox grad method}. The $t_k$ variable is used with the line-search algorithm.
		
		\begin{equation}
			x^k = prox_{g}\big( x^{(k-1)}- t_k \nabla g(x^{(k-1)})\big)
			\label{eq:prox grad method}
		\end{equation}


\section{PANOC algorithm}
	\subsection{Preconditioned FBS}
		The iteration of equation~\ref{eq:prox grad method} can indirectly be used to solve the optimization problem. Equation~\ref{eq:prox grad method} can be seen as a fixed point method. By using the residue defined in equation~\ref{eq:residue prox grad method} the fixed point can be found trough a different optimization problem. 
		\begin{equation}
			R_{\gamma}(u)= \frac{1}{\gamma}\left[ u - prox_g( u - \nabla f(u)\gamma) \right]
			\label{eq:residue prox grad method}
		\end{equation}
		The solution of equation~\ref{eq:residue prox grad method} can be found trough the Newton iteration of equation~\ref{eq:newton iteration FBS}. Where $H_k$ satisfies the inverse secant condition of equation~
		\begin{equation}
			u^{k+1} = u^k -H_kR_{\gamma}(u^k)
			\label{eq:newton iteration FBS}
		\end{equation}
		\begin{equation}
			u^{k+1} - u^k = H_{K+1} \Big( R_{\gamma}(u^{k+1})- R_{\gamma}(u^k) \Big)
		\end{equation}
	\subsection{Forward backward envelop}
		Newton iterations only converge quickly when it approaches the solution. In order to get better global behavior a proper global strategy is required. The optimization problem is changed from $\varphi = f(x) + g(x)$ to equation~\ref{eq:smoothed opti problem} This problem is smoother while it still has the same optimal solution.
		
		\begin{equation}
			\varphi_{\gamma} = f(x) - \frac{\gamma}{2}||\nabla f(x)||^2 + g^{\gamma} \big(u-\gamma \nabla f(x) \big)
			\label{eq:smoothed opti problem}
		\end{equation}
		
	\subsection{PANOC: super linear convergent algorithm}
		The PANOC algorithm is simpler then the FBE but still has the favorable properties of FBE. The basic idea is to use a convex combination of the FBS update direction and the candidate fast direction.
		
		The FBS direction is based on equation~\ref{eq:prox grad method}. The parameter $t_k$ is chosen as $\gamma$ which results in equation~\ref{eq:PANOC FBS update direction}. The nominal FBS update direction becomes $\gamma r^k$ with $r^k= \frac{u^k - \bar{u}^k}{\gamma}$.
		
		\begin{equation}
			\bar{u}^k = prox_g(u^k - \gamma \nabla_u f(u^k))
			\label{eq:PANOC FBS update direction}
		\end{equation}
		
		The candidate fast direction $d^k$ uses the curvature of the function as described in $H_k$. More formally $d^k$ becomes $d^k=-H_kr^k$.
		
		The final algorithm then becomes a convex combination of these two directions as described in equation~\ref{eq:PANOC convex combination}. The parameter $\tau$ is obtained trough line search. $\tau_k$ is largest such that $\varphi_\gamma(u^k+1) \leq \varphi_\gamma(u^k) - \sigma||r^k||^2$.
		
		\begin{equation}
			u^{k+1} = uk - (1-\tau_k)\gamma r^k + \tau_kd^k
			\label{eq:PANOC convex combination}
		\end{equation}
		
		\begin{algorithm}
			\caption{PANOC algorithm}
			\label{alg:PANOC alg}
			\begin{algorithmic}[1]
				\Procedure {PANOC}{$\gamma$,$\sigma$,$u_0$}
					\For{k=0,1,...}
						\State $\bar{u}^k = prox_g(u^k - \gamma \nabla_u f(u^k))$
						\State $r^k= \frac{u^k - \bar{u}^k}{\gamma}$
						\State $d^k=-H_kr^k$
						\State $u^{k+1} = uk - (1-\tau_k)\gamma r^k + \tau_kd^k$
					\EndFor
				\EndProcedure
			\end{algorithmic}
		\end{algorithm}
		

\end{document}
